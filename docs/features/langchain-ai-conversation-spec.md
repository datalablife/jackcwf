# LangChain v1.0 AI Conversation Feature Specification

**Feature Name**: LangChain v1.0 AI Conversation with Agents and RAG
**Version**: 1.0.0
**Priority**: P0 (Blocking - Core Feature)
**Story Points**: 13-21
**Status**: In Specification

---

## ðŸŽ¯ Constitution Alignment Checklist

**Project Constitution v1.0.0 Compliance**:
- [x] Does this feature use AI-First Architecture (Principle #1)? âœ… LangChain v1.0 `create_agent()` mandatory
- [x] Does this feature implement middleware requirements (Principle #2)? âœ… 5 core middleware layers planned
- [x] Does this feature define vector storage needs (Principle #3)? âœ… PostgreSQL + pgvector with 1536-dim vectors
- [x] Are all data models type-safe (Principle #4)? âœ… Pydantic v2 models with mypy --strict
- [x] Is this feature async-first (Principle #5)? âœ… All I/O uses async/await
- [x] Does this follow semantic organization (Principle #6)? âœ… Layered architecture documented
- [x] Is this production-ready (Principle #7)? âœ… Testing, health checks, monitoring planned
- [x] Is observability included (Principle #8)? âœ… Structured logging and metrics defined

---

## ðŸ“‹ Executive Summary

This specification defines the first production-grade feature for the cloud development platform: **LangChain v1.0 AI Conversation System**. The system enables users to engage in intelligent conversations with an AI agent powered by LangChain v1.0, with advanced capabilities including:

1. **Agent-Based Conversation** - Real-time multi-turn conversations using LangChain v1.0 `create_agent()`
2. **Agents with RAG** - Retrieval-Augmented Generation for knowledge-aware responses
3. **Vector Storage** - PostgreSQL + pgvector for semantic search and memory
4. **Middleware Architecture** - 5-layer middleware for authentication, monitoring, content moderation, etc.
5. **Type-Safe Implementation** - Full Python 3.14 type annotations + Pydantic v2 models
6. **Production Ready** - â‰¥80% test coverage, graceful shutdown, health checks

---

## ðŸŽ­ User Stories

### Story 1: User Initiates Conversation with AI Agent

```gherkin
As a user
I want to start a conversation with an AI agent
So that I can ask questions and get intelligent responses

Acceptance Criteria:
- [ ] User can send a message in natural language
- [ ] Agent receives message and processes it using LangChain v1.0
- [ ] Response appears in real-time (WebSocket streaming)
- [ ] Conversation history is persisted to database
- [ ] Response time is â‰¤ 500ms for simple queries, â‰¤ 2s for RAG queries
```

### Story 2: AI Agent Uses Tools and Perform Actions

```gherkin
As a user
I want the AI agent to use available tools (search, database, etc.)
So that it can provide comprehensive answers from multiple data sources

Acceptance Criteria:
- [ ] Agent can select appropriate tools based on user query
- [ ] Tool execution happens in parallel (asyncio.TaskGroup)
- [ ] Tool results are seamlessly integrated into response
- [ ] Tool failures are gracefully handled
- [ ] Tool execution is logged and monitored
```

### Story 3: Document Upload and RAG Integration

```gherkin
As a user
I want to upload documents for RAG
So that the agent can answer questions based on my documents

Acceptance Criteria:
- [ ] User can upload PDF/text/markdown documents
- [ ] Documents are automatically vectorized (1536-dim)
- [ ] Vectors are stored in PostgreSQL + pgvector
- [ ] Similarity search returns relevant chunks (â‰¤ 200ms)
- [ ] Agent integrates document context into responses
```

### Story 4: Conversation Memory and Context

```gherkin
As a user
I want the agent to remember previous conversations
So that I can have coherent multi-turn conversations

Acceptance Criteria:
- [ ] Previous messages are retrieved from database
- [ ] Context is injected into agent via middleware
- [ ] Agent can reference previous statements
- [ ] Memory doesn't cause token limit issues
- [ ] Summarization happens for long conversations
```

---

## ðŸ—ï¸ Technical Requirements

### 1. Data Models (Pydantic v2)

#### ChatMessage Model
```python
from pydantic import BaseModel, Field
from typing import Optional, List
from datetime import datetime
from enum import Enum

class MessageRole(str, Enum):
    USER = "user"
    ASSISTANT = "assistant"
    SYSTEM = "system"

class ChatMessageDTO(BaseModel):
    """Data transfer object for chat messages"""
    id: str = Field(..., description="Unique message ID (UUID)")
    conversation_id: str = Field(..., description="Parent conversation ID")
    role: MessageRole = Field(..., description="Message sender role")
    content: str = Field(..., description="Message text content", min_length=1, max_length=100000)
    tool_calls: Optional[List[dict]] = Field(default=None, description="Tool invocations")
    tool_results: Optional[dict] = Field(default=None, description="Tool execution results")
    tokens_used: int = Field(default=0, description="OpenAI tokens consumed")
    metadata: dict = Field(default_factory=dict, description="Custom metadata")
    created_at: datetime = Field(default_factory=datetime.utcnow)
    updated_at: datetime = Field(default_factory=datetime.utcnow)

    model_config = {
        "json_schema_extra": {
            "examples": [{
                "id": "msg_abc123",
                "conversation_id": "conv_xyz789",
                "role": "user",
                "content": "What are the latest AI trends?",
                "tokens_used": 12,
                "created_at": "2025-11-16T10:30:45Z"
            }]
        }
    }
```

#### Conversation Model
```python
class ConversationDTO(BaseModel):
    """Conversation context and metadata"""
    id: str = Field(..., description="Unique conversation ID")
    user_id: str = Field(..., description="Owner user ID")
    title: str = Field(..., description="Conversation title", min_length=1, max_length=255)
    summary: Optional[str] = Field(default=None, description="Auto-generated summary")
    model: str = Field(default="claude-sonnet-4-5-20250929", description="LLM model name")
    system_prompt: str = Field(..., description="System prompt for agent")
    metadata: dict = Field(default_factory=dict)
    is_deleted: bool = Field(default=False)
    created_at: datetime = Field(default_factory=datetime.utcnow)
    updated_at: datetime = Field(default_factory=datetime.utcnow)
```

#### Embedding Model (for RAG)
```python
class EmbeddingDTO(BaseModel):
    """Vector embedding for document chunks"""
    id: str = Field(..., description="UUID")
    document_id: str = Field(..., description="Parent document")
    chunk_text: str = Field(..., description="Actual text content")
    embedding: List[float] = Field(..., description="1536-dim vector")
    chunk_index: int = Field(..., description="Position in document")
    metadata: dict = Field(default_factory=dict, description="Source, page, etc.")
    created_at: datetime = Field(default_factory=datetime.utcnow)

    # Validation
    @property
    def embedding_dimension(self) -> int:
        return len(self.embedding)

    def model_validate_embedding(self):
        """Ensure embedding is exactly 1536 dimensions"""
        assert self.embedding_dimension == 1536, \
            f"Embedding must be 1536-dim, got {self.embedding_dimension}"
```

### 2. API Endpoints

#### Chat Endpoints

| Method | Path | Description | Auth | Rate Limit |
|--------|------|-------------|------|-----------|
| POST | `/api/v1/conversations` | Create new conversation | âœ… Required | 100/min |
| GET | `/api/v1/conversations/{id}` | Get conversation details | âœ… Required | 1000/min |
| POST | `/api/v1/conversations/{id}/messages` | Send message to agent | âœ… Required | 100/min |
| GET | `/api/v1/conversations/{id}/messages` | List conversation messages | âœ… Required | 1000/min |
| DELETE | `/api/v1/conversations/{id}` | Delete conversation (soft delete) | âœ… Required | 100/min |

#### RAG Endpoints

| Method | Path | Description | Auth | Rate Limit |
|--------|------|-------------|------|-----------|
| POST | `/api/v1/documents/upload` | Upload document for RAG | âœ… Required | 50/min |
| POST | `/api/v1/embeddings/search` | Semantic search in vectors | âœ… Required | 500/min |
| GET | `/api/v1/documents/{id}` | Get document metadata | âœ… Required | 1000/min |
| DELETE | `/api/v1/documents/{id}` | Delete document | âœ… Required | 100/min |

### 3. Database Schema

#### conversations Table
```sql
CREATE TABLE conversations (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    user_id VARCHAR NOT NULL,
    title VARCHAR(255) NOT NULL,
    summary TEXT,
    model VARCHAR(100) DEFAULT 'claude-sonnet-4-5-20250929',
    system_prompt TEXT NOT NULL,
    metadata JSONB DEFAULT '{}',
    is_deleted BOOLEAN DEFAULT FALSE,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    deleted_at TIMESTAMP,

    -- Indices for fast queries
    INDEX idx_user_created (user_id, created_at DESC),
    INDEX idx_user_active (user_id, is_deleted, created_at DESC),
    CONSTRAINT fk_user_id FOREIGN KEY (user_id) REFERENCES users(id)
);
```

#### messages Table
```sql
CREATE TABLE messages (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    conversation_id UUID NOT NULL REFERENCES conversations(id),
    role VARCHAR(10) CHECK (role IN ('user', 'assistant', 'system')),
    content TEXT NOT NULL,
    tool_calls JSONB,  -- [{"name": "...", "args": {...}}]
    tool_results JSONB,  -- Tool execution results
    tokens_used INTEGER DEFAULT 0,
    metadata JSONB DEFAULT '{}',
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,

    -- Indices
    INDEX idx_conversation (conversation_id, created_at),
    INDEX idx_role (role),
    CONSTRAINT fk_conversation FOREIGN KEY (conversation_id) REFERENCES conversations(id) ON DELETE CASCADE
);
```

#### documents Table
```sql
CREATE TABLE documents (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    user_id VARCHAR NOT NULL,
    filename VARCHAR(255) NOT NULL,
    file_type VARCHAR(20) CHECK (file_type IN ('pdf', 'txt', 'md')),
    content TEXT NOT NULL,
    total_chunks INTEGER,
    metadata JSONB DEFAULT '{}',
    is_deleted BOOLEAN DEFAULT FALSE,
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    updated_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,

    INDEX idx_user_created (user_id, created_at DESC),
    CONSTRAINT fk_user FOREIGN KEY (user_id) REFERENCES users(id)
);
```

#### embeddings Table (Vector Storage)
```sql
CREATE TABLE embeddings (
    id UUID PRIMARY KEY DEFAULT gen_random_uuid(),
    document_id UUID NOT NULL REFERENCES documents(id) ON DELETE CASCADE,
    chunk_text TEXT NOT NULL,
    embedding vector(1536) NOT NULL,  -- OpenAI Ada standard
    chunk_index INTEGER NOT NULL,
    metadata JSONB DEFAULT '{}',  -- {"page": 1, "section": "..."}
    created_at TIMESTAMP DEFAULT CURRENT_TIMESTAMP,
    is_deleted BOOLEAN DEFAULT FALSE,

    -- HNSW Index for fast similarity search
    INDEX idx_embedding USING hnsw (embedding vector_cosine_ops),
    INDEX idx_document (document_id),
    INDEX idx_created (created_at DESC)
);

-- Partitioning by month for scale
CREATE TABLE embeddings_2025_11 PARTITION OF embeddings
  FOR VALUES FROM ('2025-11-01') TO ('2025-12-01');
```

### 4. Vector Storage Requirements

**Embedding Specification**:
- Dimension: 1536 (OpenAI text-embedding-3-small standard)
- Distance Metric: cosine similarity
- Index Type: HNSW (Hierarchical Navigable Small World)
- Search Timeout: â‰¤ 200ms P99
- Batch Insert: â‰¤ 100ms per 1000 vectors

**Vectorization Pipeline**:
1. User uploads document
2. Document is chunked (1000 tokens per chunk, 200 token overlap)
3. Each chunk is vectorized via OpenAI API (text-embedding-3-small)
4. Vectors stored in PostgreSQL embeddings table with HNSW index
5. Metadata includes source document ID, page number, section

### 5. Middleware Architecture (5 Layers)

**Execution Order** (Onion Pattern):

```
Request Input
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 1. Authentication Middleware         â”‚  â† Verify JWT token
â”‚    - Extract user_id from token     â”‚
â”‚    - Check authorization            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 2. Memory Injection Middleware       â”‚  â† Retrieve context
â”‚    - Query conversation history     â”‚
â”‚    - Retrieve relevant embeddings   â”‚
â”‚    - Inject into request context    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 3. Content Moderation Middleware     â”‚  â† Safety checks
â”‚    - Check for harmful content      â”‚
â”‚    - Validate input format          â”‚
â”‚    - Rate limiting enforcement      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†“
     â–¼â–¼â–¼ BUSINESS LOGIC (Agent Processing) â–¼â–¼â–¼

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ LangChain Agent (create_agent)      â”‚
â”‚ - Tool selection                    â”‚
â”‚ - Model invocation                  â”‚
â”‚ - Tool execution                    â”‚
â”‚ - Response generation               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†‘
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 4. Response Structuring Middleware   â”‚  â† Format output
â”‚    - Serialize response to JSON     â”‚
â”‚    - Validate response schema       â”‚
â”‚    - Add metadata (tokens, etc.)    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†‘
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 5. Audit Logging Middleware         â”‚  â† Record everything
â”‚    - Log message content            â”‚
â”‚    - Log tool calls                 â”‚
â”‚    - Record performance metrics     â”‚
â”‚    - Store to database/monitoring   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
    â†‘
Response Output
```

**Implementation Example**:

```python
from langchain.agents import create_agent
from langchain.agents.middleware import AgentMiddleware
from typing import Callable

class AuthenticationMiddleware(AgentMiddleware):
    """Verify user authentication"""
    async def __call__(self, request, call_next):
        # Verify JWT token
        token = request.headers.get("Authorization")
        user_id = verify_jwt(token)
        request.state.user_id = user_id
        return await call_next(request)

class MemoryInjectionMiddleware(AgentMiddleware):
    """Inject conversation history and RAG context"""
    async def __call__(self, request, call_next):
        conversation_id = request.json().get("conversation_id")

        # Retrieve previous messages
        messages = await message_repo.get_recent(conversation_id, limit=5)

        # Retrieve relevant documents (RAG)
        user_query = request.json().get("message")
        relevant_docs = await embedding_repo.search(
            query=user_query,
            user_id=request.state.user_id,
            limit=5,
            threshold=0.7
        )

        request.state.conversation_history = messages
        request.state.rag_context = relevant_docs
        return await call_next(request)

# Create agent with middleware
agent = create_agent(
    model="claude-sonnet-4-5-20250929",
    tools=[search_web, query_documents, database_lookup],
    middleware=[
        AuthenticationMiddleware(),
        MemoryInjectionMiddleware(),
        ContentModerationMiddleware(),
        ResponseStructuringMiddleware(),
        AuditLoggingMiddleware()
    ]
)
```

### 6. Agent Tools Definition

**Built-in Tools**:

```python
from langchain.tools import tool
from typing import Annotated

@tool
def search_documents(
    query: Annotated[str, "User's search query"],
    conversation_id: Annotated[str, "Current conversation ID"]
) -> str:
    """Search uploaded documents using semantic similarity (RAG)"""
    # Implementation: Query embeddings table with HNSW
    pass

@tool
def query_database(
    sql: Annotated[str, "SQL query (SELECT only)"],
    user_id: Annotated[str, "Current user ID"]
) -> str:
    """Query user's data from PostgreSQL"""
    # Implementation: Execute safe SELECT queries
    pass

@tool
def web_search(
    query: Annotated[str, "Search query"]
) -> str:
    """Search the web for current information"""
    # Implementation: Integration with web search API
    pass

tools = [search_documents, query_database, web_search]
```

### 7. Async Implementation Details

**Core Async Components**:

```python
from sqlalchemy.ext.asyncio import AsyncSession, create_async_engine
from sqlalchemy.orm import sessionmaker
import asyncpg
import asyncio

# Async database connection
DATABASE_URL = "postgresql+asyncpg://user:pass@host:5432/db"
engine = create_async_engine(DATABASE_URL, echo=False)
AsyncSessionLocal = sessionmaker(engine, class_=AsyncSession, expire_on_commit=False)

# Async message repository
class MessageRepository:
    async def create(self, message: ChatMessageDTO) -> str:
        """Asynchronously save message to database"""
        async with AsyncSessionLocal() as session:
            db_message = MessageORM(**message.model_dump())
            session.add(db_message)
            await session.commit()
            return db_message.id

    async def get_recent(
        self,
        conversation_id: str,
        limit: int = 10
    ) -> List[ChatMessageDTO]:
        """Asynchronously retrieve recent messages"""
        async with AsyncSessionLocal() as session:
            result = await session.execute(
                select(MessageORM)
                .where(MessageORM.conversation_id == conversation_id)
                .order_by(MessageORM.created_at.desc())
                .limit(limit)
            )
            messages = result.scalars().all()
            return [ChatMessageDTO.from_orm(m) for m in messages]

# Async embedding search (vector similarity)
class EmbeddingRepository:
    async def search(
        self,
        query_text: str,
        user_id: str,
        limit: int = 5,
        threshold: float = 0.7
    ) -> List[EmbeddingDTO]:
        """
        Search embeddings using pgvector similarity
        Timeout: â‰¤ 200ms P99
        """
        # 1. Vectorize query
        query_embedding = await vectorizer.embed(query_text)

        # 2. Search with HNSW index
        async with AsyncSessionLocal() as session:
            # Using pgvector cosine similarity
            result = await session.execute(
                select(EmbeddingORM)
                .join(DocumentORM)
                .where(DocumentORM.user_id == user_id)
                .where(EmbeddingORM.embedding.cosine_distance(query_embedding) < 1 - threshold)
                .order_by(EmbeddingORM.embedding.cosine_distance(query_embedding))
                .limit(limit)
            )
            embeddings = result.scalars().all()
            return [EmbeddingDTO.from_orm(e) for e in embeddings]

# Concurrent tool execution
async def execute_agent_with_tools(
    user_message: str,
    agent: Agent,
    conversation_id: str
) -> str:
    """Execute agent with concurrent tool calls"""

    async with asyncio.TaskGroup() as tg:  # Python 3.11+
        # Parallel execution of tools if agent selects multiple
        task1 = tg.create_task(search_documents(user_message, conversation_id))
        task2 = tg.create_task(query_database("SELECT ...", user_id))

    # Results available after all complete
    doc_results, db_results = await task1, await task2

    return agent.invoke({
        "messages": [{"role": "user", "content": user_message}],
        "document_context": doc_results,
        "database_context": db_results
    })
```

### 8. Observability and Monitoring

**Structured Logging** (JSON format):

```python
import structlog
import logging
from uuid import uuid4

logger = structlog.get_logger()

# Example log entry
await logger.ainfo(
    "message_processed",
    request_id=str(uuid4()),
    user_id=user_id,
    conversation_id=conversation_id,
    message_tokens=token_count,
    response_time_ms=elapsed_time,
    tools_used=["search_documents", "query_database"],
    status="success"
)
```

**Key Metrics to Collect**:

| Metric | Target | Alert Threshold |
|--------|--------|-----------------|
| Message response time P99 | â‰¤ 500ms | > 1000ms |
| Vector search latency P99 | â‰¤ 200ms | > 300ms |
| Agent error rate | â‰¤ 1% | > 2% |
| Message tokens per request | < 2000 | > 4000 |
| Tool success rate | > 95% | < 90% |
| Database connection pool | < 80% | > 90% |

### 9. Type Safety

**Type Checking Requirements**:

```bash
# Must pass with 0 errors
mypy --strict backend/src/

# Example: Strong typing for agent creation
from typing import Callable, Any
from langchain.tools import Tool

def create_typed_agent(
    model: str,
    tools: list[Tool],
    system_prompt: str,
    middleware: list[AgentMiddleware] | None = None
) -> Agent:
    """Type-safe agent creation"""
    pass
```

---

## ðŸ§ª Testing Strategy

### Unit Tests (â‰¥ 80% coverage)

```python
# tests/unit/services/test_conversation_service.py
import pytest
from datetime import datetime

@pytest.mark.asyncio
async def test_create_conversation():
    """Test conversation creation"""
    service = ConversationService()
    conversation = await service.create(
        user_id="user_123",
        title="Test Conversation",
        system_prompt="You are a helpful assistant"
    )

    assert conversation.id is not None
    assert conversation.user_id == "user_123"
    assert conversation.title == "Test Conversation"
    assert conversation.created_at is not None

@pytest.mark.asyncio
async def test_send_message_and_get_response():
    """Test message sending and agent response"""
    service = ConversationService(agent=mock_agent)

    message = await service.send_message(
        conversation_id="conv_123",
        user_id="user_123",
        content="What is AI?"
    )

    assert message.role == "user"
    assert message.content == "What is AI?"

    # Agent should respond
    assert len(message.tool_calls) >= 0  # May or may not call tools
    assert message.tokens_used > 0

@pytest.mark.asyncio
async def test_vector_search_performance():
    """Test vector search latency"""
    repo = EmbeddingRepository()

    start_time = time.time()
    results = await repo.search(
        query_text="machine learning",
        user_id="user_123",
        limit=5
    )
    elapsed_ms = (time.time() - start_time) * 1000

    assert len(results) <= 5
    assert elapsed_ms <= 200  # Performance requirement
    assert all(r.embedding_dimension == 1536 for r in results)
```

### Integration Tests (â‰¥ 60% coverage)

```python
# tests/integration/test_agent_conversation_flow.py
@pytest.mark.asyncio
async def test_end_to_end_conversation_with_rag():
    """Test complete conversation flow with RAG"""
    # 1. Setup
    user_id = "test_user_123"

    # 2. Upload document
    doc = await document_service.upload(
        user_id=user_id,
        filename="test.pdf",
        content="Machine learning is a subset of AI..."
    )

    # 3. Create conversation
    conv = await conversation_service.create(
        user_id=user_id,
        title="ML Discussion",
        system_prompt="You are an AI expert"
    )

    # 4. Send message that requires RAG
    message = await conversation_service.send_message(
        conversation_id=conv.id,
        user_id=user_id,
        content="What does the document say about machine learning?"
    )

    # 5. Verify agent used document context
    assert any(tool == "search_documents" for tool in message.tool_calls)
    assert "machine learning" in message.content.lower()
    assert message.tokens_used > 0
```

### End-to-End Tests (Critical paths)

```python
# tests/e2e/test_ui_chat_flow.ts (Playwright)
test("user can upload document and chat with agent", async ({ page }) => {
    // 1. Login
    await page.goto("/login")
    await page.fill("[data-testid=email]", "test@example.com")
    await page.fill("[data-testid=password]", "password123")
    await page.click("[data-testid=login-btn]")

    // 2. Navigate to chat
    await page.goto("/chat")

    // 3. Upload document
    await page.click("[data-testid=upload-btn]")
    await page.setInputFiles("[data-testid=file-input]", "test.pdf")
    await expect(page.locator("[data-testid=upload-success]")).toBeVisible()

    // 4. Send message
    await page.fill("[data-testid=message-input]", "What's in the document?")
    await page.click("[data-testid=send-btn]")

    // 5. Verify response
    await expect(page.locator("[data-testid=agent-response]")).toBeVisible()
    const response = await page.textContent("[data-testid=agent-response]")
    expect(response).not.toBeEmpty()
})
```

---

## ðŸ“Š Monitoring and Performance Targets

### Response Time Targets

| Scenario | Target | P99 | Notes |
|----------|--------|-----|-------|
| Simple query (no RAG) | â‰¤ 500ms | â‰¤ 1000ms | No vector search |
| RAG query (search + respond) | â‰¤ 2000ms | â‰¤ 3000ms | Includes vector search |
| Tool execution (search_web) | â‰¤ 5000ms | â‰¤ 8000ms | Depends on external API |
| Vector search only | â‰¤ 200ms | â‰¤ 300ms | HNSW index |

### Resource Targets

| Resource | Target | Alert |
|----------|--------|-------|
| Memory per conversation | < 50MB | > 100MB |
| Database connections | < 50 | > 80 |
| Vector store size | < 10GB (1M vectors) | Growth trend |
| API response time P99 | â‰¤ 500ms | > 1000ms |

### Reliability Targets

| Metric | Target | Alert |
|--------|--------|-------|
| Agent success rate | > 99% | < 98% |
| Tool success rate | > 95% | < 90% |
| Vector search success | > 99.9% | < 99% |
| Message persistence | 100% | Any failure |

---

## ðŸ” Security and Compliance

### Input Validation
- Message content max 100,000 characters
- Tool parameters validated before execution
- SQL queries restricted to SELECT only (no DML/DDL)
- File uploads: max 50MB, allowed formats (pdf, txt, md)

### Authentication & Authorization
- JWT tokens required for all endpoints
- User can only access their own data
- Token expiration: 24 hours
- Refresh token rotation

### Data Protection
- Messages encrypted at rest (PGCRYPTO)
- Vectors not encrypted (performance)
- API communication over HTTPS only
- Audit log of all tool executions

### Content Moderation
- Prompt injection detection
- Harmful content filtering (AI-based)
- Rate limiting per user per endpoint
- PII detection and redaction option

---

## ðŸ“ˆ Deployment Checklist

### Pre-Deployment

- [ ] All unit tests pass (â‰¥ 80% coverage)
- [ ] Integration tests pass
- [ ] mypy --strict has 0 errors
- [ ] Linting (flake8, pylint) clean
- [ ] Performance tests: response times within targets
- [ ] Security audit completed
- [ ] Database migrations tested
- [ ] Vector indices created and tested

### Deployment

- [ ] Health check endpoint operational
- [ ] Graceful shutdown implemented (30s timeout)
- [ ] Monitoring dashboards active (Grafana)
- [ ] Log aggregation working (ELK Stack)
- [ ] Alerts configured for critical metrics
- [ ] Backup and disaster recovery tested
- [ ] Canary deployment (5% traffic) successful

### Post-Deployment

- [ ] Monitor error rates (< 1%)
- [ ] Check response times (P99 â‰¤ targets)
- [ ] Verify database connections healthy
- [ ] Vector search performance acceptable
- [ ] Log aggregation receiving data
- [ ] Alerts triggered and responded to
- [ ] User feedback collected

---

## ðŸ“š Technology Stack Confirmation

### Backend

| Component | Technology | Version | Purpose |
|-----------|-----------|---------|---------|
| Framework | FastAPI | Latest | API server |
| Language | Python | 3.14+ | Core implementation |
| ORM | SQLAlchemy | 2.0+ async | Database abstraction |
| Database | PostgreSQL | 15+ | Primary data store |
| Vector Store | pgvector | Latest | Vector embeddings |
| AI Framework | LangChain | v1.0 | Agent creation |
| LLM | Claude Sonnet 4.5 | Latest | Default model |
| Task Queue | Celery | 5.3+ | Async jobs |
| Cache | Redis | 7.0+ | Session/cache |
| Package Mgr | uv | 0.9+ | Dependency management |

### Frontend

| Component | Technology | Version | Purpose |
|-----------|-----------|---------|---------|
| Framework | React | 19+ | UI framework |
| Language | TypeScript | 5.7+ | Type safety |
| UI Library | Tailark | Latest | Components |
| State Mgmt | Zustand | Latest | Global state |
| Data Fetch | TanStack Query | Latest | Server state |
| Forms | React Hook Form | Latest | Form handling |
| Real-time | Socket.IO | Latest | WebSocket |
| Build Tool | Vite | Latest | Fast builds |

### DevOps

| Component | Technology | Purpose |
|-----------|-----------|---------|
| Deployment | Coolify | Self-hosted PaaS |
| Container | Docker | Containerization |
| CI/CD | GitHub Actions | Automation |
| Monitoring | Prometheus | Metrics collection |
| Visualization | Grafana | Dashboards |
| Logging | ELK Stack | Log aggregation |
| Database Backup | pg_dump | Daily backups |

---

## ðŸ“… Implementation Timeline

### Phase 1: Backend Infrastructure (Week 1-2)
- [ ] Database schema setup (conversations, messages, documents, embeddings)
- [ ] Async repository layer (SQLAlchemy async)
- [ ] Vector search implementation (pgvector, HNSW)
- [ ] Basic API endpoints scaffold

### Phase 2: Agent Integration (Week 2-3)
- [ ] LangChain v1.0 setup
- [ ] Agent creation (create_agent)
- [ ] Tool definitions (search_documents, query_database, web_search)
- [ ] RAG pipeline (chunking, vectorization, retrieval)

### Phase 3: Middleware & Features (Week 3-4)
- [ ] Implement 5-layer middleware
- [ ] Authentication and authorization
- [ ] Message history and context injection
- [ ] Real-time WebSocket streaming

### Phase 4: Frontend (Week 4-5)
- [ ] Chat UI with Tailark components
- [ ] Document upload interface
- [ ] Conversation management
- [ ] Real-time message updates

### Phase 5: Testing & Optimization (Week 5-6)
- [ ] Unit tests (â‰¥ 80% coverage)
- [ ] Integration tests (â‰¥ 60% coverage)
- [ ] Performance testing and optimization
- [ ] Load testing (target: 100 concurrent users)

### Phase 6: Deployment (Week 6-7)
- [ ] CI/CD pipeline setup
- [ ] Staging deployment
- [ ] Monitoring and alerting
- [ ] Production deployment (canary â†’ full)

---

## ðŸš€ Success Criteria

### Functional Requirements
- [x] Users can create conversations with AI agent
- [x] Agent responds to natural language queries
- [x] Agent can use multiple tools (search, database, web)
- [x] Users can upload documents for RAG
- [x] Vector search works with â‰¤ 200ms latency
- [x] Conversation history is preserved
- [x] Type-safe implementation (mypy --strict passes)

### Performance Requirements
- [x] Simple query response: â‰¤ 500ms
- [x] RAG query response: â‰¤ 2000ms
- [x] Vector search: â‰¤ 200ms
- [x] 100+ concurrent users supported

### Quality Requirements
- [x] Unit test coverage â‰¥ 80%
- [x] Integration test coverage â‰¥ 60%
- [x] Zero critical security vulnerabilities
- [x] Error rate â‰¤ 1%
- [x] Uptime: 99.5%

### Code Quality
- [x] All functions have type annotations
- [x] mypy --strict: 0 errors
- [x] Async/await for all I/O
- [x] Structured logging (JSON)
- [x] Comprehensive monitoring

---

## ðŸ“– References and Resources

### LangChain v1.0
- **Overview**: https://docs.langchain.com/oss/python/releases/langchain-v1
- **Agent Creation**: `from langchain.agents import create_agent`
- **Middleware**: `from langchain.agents.middleware import AgentMiddleware`
- **Content Blocks**: `response.content_blocks` (unified API)

### Vector Search
- **pgvector GitHub**: https://github.com/pgvector/pgvector
- **HNSW Index**: Hierarchical Navigable Small World algorithm
- **Embedding Models**: OpenAI text-embedding-3-small (1536 dimensions)

### UI Components
- **Tailark**: https://tailark.com/hero-section
- **Hero Section**: Landing and conversation starter
- **Chat Interface**: Message display, input form
- **File Upload**: Document upload UI

### Database & ORM
- **SQLAlchemy Async**: https://docs.sqlalchemy.org/en/20/orm/extensions/asyncio.html
- **PostgreSQL Async**: asyncpg driver
- **Alembic**: Database migration tool

### LLM Models
- **Claude Sonnet 4.5**: Latest Anthropic model (default)
- **OpenAI GPT-4o**: Alternative model
- **Google Gemini**: Multi-modal support
- **AWS Bedrock**: Enterprise option

---

## ðŸ“ Notes and Assumptions

1. **User Authentication** - Assumes existing user system with JWT tokens
2. **PostgreSQL Setup** - Assumes PostgreSQL 15+ with pgvector extension installed
3. **LLM API Keys** - Assumes environment variables for API access
4. **File Storage** - Documents stored in database; can migrate to S3 later
5. **Vector Dimension** - Fixed at 1536 (OpenAI standard)
6. **Model Selection** - Defaults to Claude Sonnet 4.5; configurable per user
7. **Rate Limiting** - IP-based and user-based; configured per endpoint
8. **Backup Strategy** - Daily PostgreSQL backups; vectors included

---

**Document Version**: 1.0.0
**Last Updated**: 2025-11-16
**Status**: Ready for Implementation
**Next Step**: Proceed to Planning Phase (plan-template.md)
