# Phase 2 æµå¼ LLM å“åº”ä¼˜åŒ– - å®ç°è§„åˆ’

**å¯åŠ¨æ—¶é—´**: 2025-11-20
**é¢„æœŸå®Œæˆ**: 2025-11-21 (4 å°æ—¶)
**ä¼˜å…ˆçº§**: P0 (å…³é”®)
**é¢„æœŸæˆæœ**: é¦–å­—èŠ‚å»¶è¿Ÿ -81% (550ms â†’ 100ms)

---

## ğŸ¯ æ‰§è¡Œæ‘˜è¦

Phase 2 æµå¼ LLM å“åº”ä¼˜åŒ–æ—¨åœ¨é€šè¿‡ Server-Sent Events (SSE) å®ç°å®æ—¶æµå¼ä¼ è¾“ï¼Œå¤§å¹…æ”¹å–„ç”¨æˆ·æ„ŸçŸ¥å»¶è¿Ÿã€‚è¿™æ˜¯æå‡ç”¨æˆ·ä½“éªŒçš„å…³é”®ä¼˜åŒ–ã€‚

### å…³é”®æŒ‡æ ‡

| æŒ‡æ ‡ | å½“å‰ | ç›®æ ‡ | æ”¹è¿› |
|------|------|------|------|
| **é¦–å­—èŠ‚å»¶è¿Ÿ** | 550ms | 100ms | **-81%** âœ… |
| **ç”¨æˆ·æ„ŸçŸ¥å»¶è¿Ÿ** | é«˜ | ä½ | **æ˜¾è‘—** âœ… |
| **å—ååé‡** | N/A | >50/sec | **æ–°å¢** âœ… |
| **å†…å­˜å ç”¨** | N/A | <20MB | **ç›®æ ‡** âœ… |

---

## ğŸ“ æŠ€æœ¯æ¶æ„è®¾è®¡

### æµå¼å“åº”ç®¡é“

```
ç”¨æˆ·è¯·æ±‚
    â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ POST /api/v1/conversations/{id}/stream  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ StreamingChatService                    â”‚
â”‚  â”œâ”€ åˆå§‹åŒ–å¯¹è¯ä¸Šä¸‹æ–‡                     â”‚
â”‚  â”œâ”€ åŠ è½½ç¼“å­˜çš„æ¶ˆæ¯å†å²                   â”‚
â”‚  â””â”€ å‡†å¤‡ LangChain Agent                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ LangChain Agent (æµå¼æ¨¡å¼)                â”‚
â”‚  â”œâ”€ æµå¼è¾“å…¥å¤„ç†                         â”‚
â”‚  â”œâ”€ å·¥å…·è°ƒç”¨æµå¼è¾“å‡º                     â”‚
â”‚  â””â”€ æœ€ç»ˆå“åº”ç”Ÿæˆ                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ SSE äº‹ä»¶ç¼“å†²åŒº                            â”‚
â”‚  â”œâ”€ message_chunk (50-100 tokens)       â”‚
â”‚  â”œâ”€ tool_call                           â”‚
â”‚  â”œâ”€ tool_result                         â”‚
â”‚  â””â”€ complete_state                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ HTTP Response (SSE)                      â”‚
â”‚  data: {"type": "message_chunk", ...}   â”‚
â”‚  data: {"type": "tool_call", ...}       â”‚
â”‚  data: {"type": "complete_state", ...}  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
             â†“
æµè§ˆå™¨ (å®æ—¶æ¸²æŸ“)
```

### æ ¸å¿ƒç»„ä»¶

```
src/
â”œâ”€ api/
â”‚  â”œâ”€ streaming_routes.py          (æ–°å»º - SSE ç«¯ç‚¹)
â”‚  â”‚  â””â”€ POST /api/v1/conversations/{id}/stream
â”‚  â”‚  â””â”€ OPTIONS /api/v1/conversations/{id}/stream
â”‚  â”‚
â”‚  â””â”€ existing routes (ä¿®æ”¹)
â”‚     â””â”€ ä¿ç•™éæµå¼ç«¯ç‚¹å‘åå…¼å®¹
â”‚
â”œâ”€ services/
â”‚  â”œâ”€ streaming_chat_service.py    (æ–°å»º - æµå¼èŠå¤©æœåŠ¡)
â”‚  â”‚  â””â”€ StreamingChatService
â”‚  â”‚  â”œâ”€ stream_agent_response()
â”‚  â”‚  â”œâ”€ _build_streaming_agent()
â”‚  â”‚  â””â”€ _format_stream_event()
â”‚  â”‚
â”‚  â””â”€ existing services (ä¿®æ”¹)
â”‚     â””â”€ SemanticCacheService é›†æˆ
â”‚     â””â”€ AgentService æµå¼æ”¯æŒ
â”‚
â”œâ”€ models/
â”‚  â”œâ”€ streaming_models.py          (æ–°å»º - æ•°æ®æ¨¡å‹)
â”‚  â”‚  â””â”€ StreamEvent
â”‚  â”‚  â”œâ”€ MessageChunkEvent
â”‚  â”‚  â”œâ”€ ToolCallEvent
â”‚  â”‚  â”œâ”€ ToolResultEvent
â”‚  â”‚  â””â”€ CompleteStateEvent
â”‚  â”‚
â”‚  â””â”€ existing models (å…¼å®¹)
â”‚
â””â”€ infrastructure/
   â”œâ”€ streaming_metrics.py          (æ–°å»º - æµå¼æŒ‡æ ‡)
   â”‚  â””â”€ é¦–å­—èŠ‚å»¶è¿Ÿè¿½è¸ª
   â”‚  â””â”€ å—ååé‡è®¡æ•°
   â”‚  â””â”€ å†…å­˜ä½¿ç”¨ç›‘æ§
   â”‚
   â””â”€ existing metrics (æ‰©å±•)
      â””â”€ Prometheus æŒ‡æ ‡é›†æˆ
```

---

## ğŸ› ï¸ å®ç°æ­¥éª¤ (4 å°æ—¶)

### Step 1: åˆ›å»ºæ•°æ®æ¨¡å‹ (30 åˆ†é’Ÿ)

**æ–‡ä»¶**: `src/models/streaming_models.py`

```python
from typing import Any, Dict, List, Optional
from pydantic import BaseModel, Field
from enum import Enum

class StreamEventType(str, Enum):
    """æµå¼äº‹ä»¶ç±»å‹"""
    MESSAGE_CHUNK = "message_chunk"        # æ–‡æœ¬å—
    TOOL_CALL = "tool_call"                # å·¥å…·è°ƒç”¨
    TOOL_RESULT = "tool_result"            # å·¥å…·ç»“æœ
    THINKING = "thinking"                  # æ€è€ƒè¿‡ç¨‹
    COMPLETE_STATE = "complete_state"      # å®ŒæˆçŠ¶æ€
    ERROR = "error"                        # é”™è¯¯

class StreamEvent(BaseModel):
    """åŸºç¡€æµå¼äº‹ä»¶"""
    type: StreamEventType
    timestamp: float
    sequence: int

class MessageChunkEvent(StreamEvent):
    """æ¶ˆæ¯å—äº‹ä»¶"""
    type: StreamEventType = StreamEventType.MESSAGE_CHUNK
    content: str                            # æ–‡æœ¬å—å†…å®¹
    token_count: int                        # Token æ•°
    is_final: bool = False                 # æ˜¯å¦æœ€åä¸€å—

class ToolCallEvent(StreamEvent):
    """å·¥å…·è°ƒç”¨äº‹ä»¶"""
    type: StreamEventType = StreamEventType.TOOL_CALL
    tool_name: str                         # å·¥å…·åç§°
    tool_input: Dict[str, Any]             # å·¥å…·è¾“å…¥å‚æ•°

class ToolResultEvent(StreamEvent):
    """å·¥å…·ç»“æœäº‹ä»¶"""
    type: StreamEventType = StreamEventType.TOOL_RESULT
    tool_name: str
    result: Any                            # å·¥å…·æ‰§è¡Œç»“æœ

class CompleteStateEvent(StreamEvent):
    """å®ŒæˆçŠ¶æ€äº‹ä»¶"""
    type: StreamEventType = StreamEventType.COMPLETE_STATE
    final_message: str                     # æœ€ç»ˆæ¶ˆæ¯
    total_tokens: int                      # æ€» Token æ•°
    total_chunks: int                      # æ€»å—æ•°
    elapsed_time: float                    # è€—æ—¶ (ç§’)
```

### Step 2: åˆ›å»ºæµå¼èŠå¤©æœåŠ¡ (90 åˆ†é’Ÿ)

**æ–‡ä»¶**: `src/services/streaming_chat_service.py`

```python
import asyncio
import json
import time
from typing import AsyncGenerator, Dict, Any, Optional
from langchain_core.callbacks.base import BaseCallbackHandler
from langchain_core.messages import HumanMessage
from src.models.streaming_models import (
    StreamEvent, MessageChunkEvent, ToolCallEvent,
    ToolResultEvent, CompleteStateEvent, StreamEventType
)
from src.services.agent_service import AgentService
from src.services.semantic_cache import SemanticCacheService
import logging

logger = logging.getLogger(__name__)

class StreamingCallbackHandler(BaseCallbackHandler):
    """LangChain æµå¼å›è°ƒå¤„ç†å™¨"""

    def __init__(self):
        self.chunks = []
        self.current_content = ""

    async def on_llm_new_token(self, token: str, **kwargs) -> None:
        """å¤„ç†æ–° Token"""
        self.current_content += token
        self.chunks.append(token)

    async def on_tool_start(self, tool: str, input: Dict[str, Any], **kwargs) -> None:
        """å·¥å…·è°ƒç”¨å¼€å§‹"""
        logger.info(f"Tool started: {tool}")

    async def on_tool_end(self, output: str, **kwargs) -> None:
        """å·¥å…·è°ƒç”¨ç»“æŸ"""
        logger.info(f"Tool ended with output: {output[:100]}")

class StreamingChatService:
    """æµå¼èŠå¤©æœåŠ¡"""

    def __init__(self, agent_service: AgentService, cache_service: SemanticCacheService):
        self.agent_service = agent_service
        self.cache_service = cache_service
        self.chunk_buffer = []
        self.buffer_size = 50  # Token æ•°

    async def stream_agent_response(
        self,
        conversation_id: str,
        user_message: str,
        user_id: str,
    ) -> AsyncGenerator[StreamEvent, None]:
        """
        æµå¼ç”Ÿæˆ Agent å“åº”

        é¦–å­—èŠ‚å»¶è¿Ÿç›®æ ‡: <100ms
        å—ååé‡ç›®æ ‡: >50 chunks/sec
        """

        start_time = time.time()
        sequence = 0
        total_tokens = 0
        total_chunks = 0

        try:
            # 1. åŠ è½½å¯¹è¯å†å²
            messages = await self._load_conversation_messages(conversation_id)

            # 2. é¦–å­—èŠ‚å»¶è¿Ÿ: åœ¨å®é™…ç”Ÿæˆå‰å‘é€åˆå§‹åŒ–äº‹ä»¶
            first_byte_time = time.time() - start_time
            logger.info(f"First byte latency: {first_byte_time:.1f}ms")

            # 3. å»ºç«‹æµå¼ Agent
            agent = self._build_streaming_agent()
            callback_handler = StreamingCallbackHandler()

            # 4. æ‰§è¡Œ Agent å¹¶æµå¼å¤„ç†è¾“å‡º
            accumulated_tokens = ""
            async for chunk in agent.astream_events(
                input={"messages": messages + [HumanMessage(content=user_message)]},
                config={"callbacks": [callback_handler]},
            ):
                event = chunk

                # å¤„ç† LLM äº‹ä»¶
                if event.get("event") == "on_chat_model_stream":
                    content = event.get("data", {}).get("chunk", {}).get("content", "")
                    if content:
                        accumulated_tokens += content

                        # ç¼“å†² Token ç›´åˆ°è¾¾åˆ°ç¼“å†²å¤§å°
                        if len(accumulated_tokens) >= self.buffer_size:
                            chunk_event = MessageChunkEvent(
                                type=StreamEventType.MESSAGE_CHUNK,
                                timestamp=time.time(),
                                sequence=sequence,
                                content=accumulated_tokens,
                                token_count=len(accumulated_tokens.split()),
                                is_final=False,
                            )
                            sequence += 1
                            total_tokens += chunk_event.token_count
                            total_chunks += 1
                            yield chunk_event
                            accumulated_tokens = ""

                # å¤„ç†å·¥å…·è°ƒç”¨
                elif event.get("event") == "on_tool_start":
                    tool_event = ToolCallEvent(
                        type=StreamEventType.TOOL_CALL,
                        timestamp=time.time(),
                        sequence=sequence,
                        tool_name=event.get("data", {}).get("tool", ""),
                        tool_input=event.get("data", {}).get("input", {}),
                    )
                    sequence += 1
                    yield tool_event

                # å¤„ç†å·¥å…·ç»“æœ
                elif event.get("event") == "on_tool_end":
                    result_event = ToolResultEvent(
                        type=StreamEventType.TOOL_RESULT,
                        timestamp=time.time(),
                        sequence=sequence,
                        tool_name=event.get("data", {}).get("tool", ""),
                        result=event.get("data", {}).get("output", ""),
                    )
                    sequence += 1
                    yield result_event

            # å‘é€å‰©ä½™çš„æ–‡æœ¬å—
            if accumulated_tokens:
                final_chunk = MessageChunkEvent(
                    type=StreamEventType.MESSAGE_CHUNK,
                    timestamp=time.time(),
                    sequence=sequence,
                    content=accumulated_tokens,
                    token_count=len(accumulated_tokens.split()),
                    is_final=True,
                )
                sequence += 1
                total_tokens += final_chunk.token_count
                total_chunks += 1
                yield final_chunk

            # å‘é€å®ŒæˆçŠ¶æ€
            elapsed = time.time() - start_time
            complete_event = CompleteStateEvent(
                type=StreamEventType.COMPLETE_STATE,
                timestamp=time.time(),
                sequence=sequence,
                final_message=accumulated_tokens,
                total_tokens=total_tokens,
                total_chunks=total_chunks,
                elapsed_time=elapsed,
            )
            yield complete_event

            logger.info(
                f"Stream complete: {total_tokens} tokens, "
                f"{total_chunks} chunks, {elapsed:.1f}s elapsed"
            )

        except Exception as e:
            logger.error(f"Streaming error: {e}", exc_info=True)
            error_event = StreamEvent(
                type=StreamEventType.ERROR,
                timestamp=time.time(),
                sequence=sequence,
            )
            yield error_event

    def _build_streaming_agent(self):
        """æ„å»ºæ”¯æŒæµå¼çš„ Agent"""
        # ä½¿ç”¨ç°æœ‰çš„ agent_serviceï¼Œé…ç½®ä¸ºæµå¼æ¨¡å¼
        return self.agent_service.create_agent(streaming=True)

    async def _load_conversation_messages(self, conversation_id: str):
        """åŠ è½½å¯¹è¯æ¶ˆæ¯å†å²"""
        # TODO: ä»æ•°æ®åº“åŠ è½½
        return []
```

### Step 3: åˆ›å»º SSE ç«¯ç‚¹ (90 åˆ†é’Ÿ)

**æ–‡ä»¶**: `src/api/streaming_routes.py`

```python
from fastapi import APIRouter, HTTPException, Depends
from fastapi.responses import StreamingResponse
import json
import asyncio
from typing import AsyncGenerator
from src.services.streaming_chat_service import StreamingChatService
from src.services.agent_service import AgentService
from src.services.semantic_cache import SemanticCacheService
from src.db.config import get_async_session
from src.middleware.auth_middleware import get_current_user
import logging

logger = logging.getLogger(__name__)

router = APIRouter(prefix="/api/v1", tags=["streaming"])

@router.post("/conversations/{conversation_id}/stream")
async def stream_conversation(
    conversation_id: str,
    message: dict,
    user_id: str = Depends(get_current_user),
    session = Depends(get_async_session),
):
    """
    æµå¼ API ç«¯ç‚¹ - ä½¿ç”¨ Server-Sent Events

    è¯·æ±‚ä½“:
    {
        "message": "ç”¨æˆ·æ¶ˆæ¯"
    }

    å“åº”: Server-Sent Events æµ
    data: {"type": "message_chunk", "content": "...", "sequence": 0}
    data: {"type": "tool_call", "tool_name": "...", "sequence": 1}
    data: {"type": "complete_state", "total_tokens": 100, ...}
    """

    # éªŒè¯è¾“å…¥
    if not message.get("message"):
        raise HTTPException(status_code=400, detail="Missing message field")

    # åˆå§‹åŒ–æœåŠ¡
    agent_service = AgentService()
    cache_service = SemanticCacheService()
    streaming_service = StreamingChatService(agent_service, cache_service)

    async def event_generator() -> AsyncGenerator[str, None]:
        """SSE äº‹ä»¶ç”Ÿæˆå™¨"""
        try:
            async for event in streaming_service.stream_agent_response(
                conversation_id=conversation_id,
                user_message=message["message"],
                user_id=user_id,
            ):
                # è½¬æ¢ä¸º JSON å¹¶å‘é€
                event_json = json.dumps(event.dict())
                yield f"data: {event_json}\n\n"

                # ç»™å®¢æˆ·ç«¯æ—¶é—´å¤„ç†äº‹ä»¶
                await asyncio.sleep(0.01)

        except Exception as e:
            logger.error(f"Event generation error: {e}", exc_info=True)
            error_json = json.dumps({
                "type": "error",
                "message": str(e),
            })
            yield f"data: {error_json}\n\n"

    return StreamingResponse(
        event_generator(),
        media_type="text/event-stream",
        headers={
            "Cache-Control": "no-cache",
            "X-Accel-Buffering": "no",  # ç¦ç”¨ä»£ç†ç¼“å†²
            "Connection": "keep-alive",
        },
    )

@router.options("/conversations/{conversation_id}/stream")
async def options_stream(conversation_id: str):
    """CORS é¢„æ£€è¯·æ±‚"""
    return {
        "allow": ["POST", "OPTIONS"],
    }
```

### Step 4: é›†æˆåˆ°ä¸»åº”ç”¨ (30 åˆ†é’Ÿ)

**æ–‡ä»¶**: `src/main.py` (ä¿®æ”¹)

```python
# åœ¨è·¯ç”±æ³¨å†Œéƒ¨åˆ†æ·»åŠ 
from src.api.streaming_routes import router as streaming_router

# ... å…¶ä»–ä»£ç  ...

def register_routes():
    """æ³¨å†Œæ‰€æœ‰ API è·¯ç”±"""
    # ... ç°æœ‰è·¯ç”± ...

    # æ·»åŠ æµå¼è·¯ç”±
    app.include_router(streaming_router)
    logger.info("Registered streaming routes")
```

### Step 5: åˆ›å»ºç›‘æ§æŒ‡æ ‡ (30 åˆ†é’Ÿ)

**æ–‡ä»¶**: `src/infrastructure/streaming_metrics.py` (æ–°å»º)

```python
from prometheus_client import Histogram, Counter, Gauge
import logging

logger = logging.getLogger(__name__)

# é¦–å­—èŠ‚å»¶è¿Ÿç›´æ–¹å›¾ (æ¯«ç§’)
first_byte_latency = Histogram(
    "streaming_first_byte_latency_ms",
    "First byte latency for streaming responses",
    buckets=(10, 25, 50, 100, 250, 500, 1000),
)

# å—ååé‡è®¡æ•°å™¨
chunk_throughput = Counter(
    "streaming_chunks_total",
    "Total chunks sent in streaming responses",
)

# æ´»è·ƒæµè¿æ¥æ•°
active_streams = Gauge(
    "streaming_active_connections",
    "Number of active streaming connections",
)

# æµå®Œæˆè®¡æ•°å™¨
stream_completions = Counter(
    "streaming_completions_total",
    "Total completed streaming responses",
    labelnames=["status"],  # success, error, timeout
)

# æµå†…å­˜ä½¿ç”¨
stream_memory_usage = Gauge(
    "streaming_memory_mb",
    "Memory usage for streaming buffers",
)
```

---

## âœ… éªŒè¯æ¸…å•

### ä»£ç å®Œæˆæ¸…å•
- [ ] `src/models/streaming_models.py` - æ•°æ®æ¨¡å‹å®Œæˆ
- [ ] `src/services/streaming_chat_service.py` - æµå¼æœåŠ¡å®Œæˆ
- [ ] `src/api/streaming_routes.py` - SSE ç«¯ç‚¹å®Œæˆ
- [ ] `src/infrastructure/streaming_metrics.py` - ç›‘æ§æŒ‡æ ‡å®Œæˆ
- [ ] `src/main.py` - è·¯ç”±é›†æˆå®Œæˆ

### åŠŸèƒ½éªŒè¯æ¸…å•
- [ ] SSE ç«¯ç‚¹è¿”å›æµå¼å“åº”
- [ ] æ¶ˆæ¯å—æ­£ç¡®åºåˆ—åŒ–
- [ ] å·¥å…·è°ƒç”¨äº‹ä»¶æ­£ç¡®å‘é€
- [ ] å®ŒæˆçŠ¶æ€äº‹ä»¶åŒ…å«æ­£ç¡®ä¿¡æ¯
- [ ] é”™è¯¯å¤„ç†å’Œæ—¥å¿—è®°å½•å®Œæ•´

### æ€§èƒ½éªŒè¯æ¸…å•
- [ ] é¦–å­—èŠ‚å»¶è¿Ÿ < 100ms (ç›®æ ‡)
- [ ] å—ååé‡ > 50 chunks/sec (ç›®æ ‡)
- [ ] å†…å­˜å ç”¨ < 20MB per connection (ç›®æ ‡)
- [ ] æ— å†…å­˜æ³„æ¼ (é•¿è¿æ¥æµ‹è¯•)
- [ ] å¹¶å‘è¿æ¥æ­£ç¡®å¤„ç† (10+ å¹¶å‘)

### ç›‘æ§éªŒè¯æ¸…å•
- [ ] Prometheus æŒ‡æ ‡æ­£ç¡®å¯¼å‡º
- [ ] é¦–å­—èŠ‚å»¶è¿Ÿç›´æ–¹å›¾è®°å½•
- [ ] æ´»è·ƒè¿æ¥è®¡æ•°æ­£ç¡®
- [ ] å®ŒæˆçŠ¶æ€åˆ†ç±»ç»Ÿè®¡

---

## ğŸ§ª æµ‹è¯•è®¡åˆ’ (1 å°æ—¶)

### å•å…ƒæµ‹è¯•
```bash
# æµ‹è¯•æµå¼æœåŠ¡
pytest tests/test_streaming_service.py -v

# æµ‹è¯•æ•°æ®æ¨¡å‹åºåˆ—åŒ–
pytest tests/test_streaming_models.py -v
```

### é›†æˆæµ‹è¯•
```bash
# æµ‹è¯• SSE ç«¯ç‚¹
pytest tests/integration/test_streaming_endpoint.py -v

# æµ‹è¯•æµå¼ Agent é›†æˆ
pytest tests/integration/test_streaming_agent.py -v
```

### æ€§èƒ½æµ‹è¯•
```bash
# æµ‹è¯•é¦–å­—èŠ‚å»¶è¿Ÿ
locust -f tests/load_test_streaming.py \
        --host=http://localhost:8000 \
        -u 20 -r 5 -t 5m

# æµ‹è¯•å¹¶å‘è¿æ¥
curl -N http://localhost:8000/api/v1/conversations/{id}/stream \
     -d '{"message": "test"}' \
     -H "Content-Type: application/json" \
     -H "Authorization: Bearer $TOKEN"
```

---

## ğŸ“Š æ€§èƒ½ç›®æ ‡

| æŒ‡æ ‡ | ç›®æ ‡ | æ–¹æ³• |
|------|------|------|
| **é¦–å­—èŠ‚å»¶è¿Ÿ** | <100ms | åœ¨ LLM è¿”å›ç¬¬ä¸€ä¸ª token å‰æœ€å°åŒ–å¤„ç† |
| **å—ååé‡** | >50/sec | ä¼˜åŒ–ç¼“å†²åŒºå¤§å°å’Œå‘é€é¢‘ç‡ |
| **å†…å­˜ä½¿ç”¨** | <20MB | é™åˆ¶ç¼“å†²åŒºå¤§å°ï¼ŒåŠæ—¶é‡Šæ”¾ |
| **å¹¶å‘è¿æ¥** | 10+ | ä½¿ç”¨å¼‚æ­¥ I/Oï¼Œé¿å…é˜»å¡ |
| **é”™è¯¯æ¢å¤** | è‡ªåŠ¨é‡è¯• | å®ç°é‡è¯•é€»è¾‘å’Œé™çº§ç­–ç•¥ |

---

## ğŸ“… æ—¶é—´çº¿

```
Day 1 (ä»Šå¤©):
â”œâ”€ Step 1: åˆ›å»ºæ•°æ®æ¨¡å‹ (30min) âœ… é¢„æœŸå®Œæˆ
â”œâ”€ Step 2: åˆ›å»ºæµå¼æœåŠ¡ (90min) âœ… é¢„æœŸå®Œæˆ
â”œâ”€ Step 3: åˆ›å»º SSE ç«¯ç‚¹ (90min) âœ… é¢„æœŸå®Œæˆ
â”œâ”€ Step 4: é›†æˆåˆ°ä¸»åº”ç”¨ (30min) âœ… é¢„æœŸå®Œæˆ
â””â”€ Step 5: åˆ›å»ºç›‘æ§æŒ‡æ ‡ (30min) âœ… é¢„æœŸå®Œæˆ

Day 2 (æ˜å¤©):
â”œâ”€ å•å…ƒ/é›†æˆæµ‹è¯•
â”œâ”€ æ€§èƒ½æµ‹è¯•å’ŒåŸºå‡†
â””â”€ å®ŒæˆæŠ¥å‘Šç”Ÿæˆ

Total: 4 å°æ—¶å·¥ä½œ + 2-3 å°æ—¶æµ‹è¯• = Phase 2 å®Œæˆ âœ…
```

---

## ğŸš€ åç»­æ­¥éª¤

### ç«‹å³ (ä»Šå¤©)
```
[ ] å¼€å§‹ Step 1: åˆ›å»ºæ•°æ®æ¨¡å‹
[ ] Step 1 å®Œæˆå â†’ Step 2
[ ] Step 2 å®Œæˆå â†’ Step 3
[ ] å®Œæˆæ‰€æœ‰ Step 1-5
```

### æ˜å¤©
```
[ ] è¿è¡Œå•å…ƒ/é›†æˆæµ‹è¯•
[ ] æ‰§è¡Œæ€§èƒ½åŸºå‡†æµ‹è¯•
[ ] ç”Ÿæˆ Phase 2 å®ŒæˆæŠ¥å‘Š
[ ] å¯åŠ¨ Phase 3 (Claude Prompt Caching)
```

---

**Phase 2 å·²å‡†å¤‡å°±ç»ªï¼é¦–å­—èŠ‚å»¶è¿Ÿæ”¹è¿›ç›®æ ‡ -81% (550ms â†’ 100ms)**

éœ€è¦æˆ‘ç°åœ¨å¼€å§‹å®ç° Step 1 å—ï¼Ÿ
